All nodes: ( Note: always execute the command primary node first)
---------
yum install -y pcs
yum install nfs-utils -y
yum install -y policycoreutils-python
yum -y install wget git
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
wget http://www.elrepo.org/elrepo-release-7.0-5.el7.elrepo.noarch.rpm
rpm -i epel-release-7-14.noarch.rpm
yum -y install drbd90-utils kmod-drbd90 ntp ntpdate
 
wget https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-14.noarch.rpm
rpm -Uvh epel-release*rpm
yum install dkms
 
firewall-cmd --permanent --add-service=high-availability
firewall-cmd --add-service=high-availability
 
sed -i "s/^SELINUX=.*/SELINUX=permissive/g" /etc/selinux/config
setenforce 0
semanage permissive -a drbd_t
 
 
pvcreate /dev/sdb
vgcreate datavg /dev/sdb
lvcreate -l 100%VG  -n lvnfs001  datavg
 
echo "h1gHs3cret" | passwd hacluster --stdin
 
 
vim /etc/drbd.d/d0.res
 
resource d0 {
        net {
                protocol C;
                max-buffers             36k;
                sndbuf-size            1024k ;
                rcvbuf-size            2048k;
        }
        device /dev/drbd0;
        disk /dev/datavg/lvnfs001;
        meta-disk internal;
        on xxxxxxxxx {
                address X.X.X.X:7788;
                node-id 0;
        }
        on xxxxxxxxx {
                address xxxxxxxxx:7788;
                node-id 1;
        }
        disk {
                resync-rate 512M;
                on-io-error detach;
                no-disk-flushes ;
                no-disk-barrier;
                c-plan-ahead 0;
                c-fill-target 24M;
                c-min-rate 80M;
                c-max-rate 720M;
        }
        connection-mesh {
                hosts cluster1 cluster2;
        }
}
#------------------------
 
systemctl enable pcsd
systemctl enable corosync
systemctl enable pacemaker
systemctl start pcsd
systemctl start corosync
systemctl start pacemaker
 
drbdadm create-md d0
drbdadm down d0
drbdadm up d0
 
 
Note:service start up errors:
https://bugzilla.redhat.com/show_bug.cgi?id=1273288
 
 
Only on Master node:
-------------------
drbdadm --force primary d0
mkfs.xfs /dev/drbd0
mount /dev/drbd0 /mnt
chmod -R 777 /mnt
umount /mnt
 
Only on Slave node:
-------------------
drbdadm secondary  d0
drbdadm status
 
 
All nodes:
---------
disable proxy now
 
 
Only on Master node:
-------------------
pcs cluster auth cluster1 cluster1
pcs cluster setup --force --name nfs_cluster cluster1 cluster1
pcs cluster start --all
pcs cluster enable --all
pcs property set stonith-enabled=false
pcs property set no-quorum-policy=ignore
 
pcs resource create nfs_vip ocf:heartbeat:IPaddr2 ip=10.125.89.253 nic=ens160 cidr_netmask=32 op monitor interval=10s
pcs resource defaults resource-stickiness=100
 
 
systemctl start nfs
systemctl enable nfs
 
DRBDRES=d0
pcs cluster cib add_drbd
pcs -f add_drbd resource create nfsserver_data ocf:linbit:drbd drbd_resource=$DRBDRES op monitor interval=60s
pcs -f add_drbd resource master nfsserver_data_sync nfsserver_data master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
pcs cluster cib-push add_drbd
rm -rf add_drbd
 
pcs cluster cib fs_cfg
pcs -f fs_cfg resource create nfsfs Filesystem device="/dev/drbd0" directory="/data" fstype="xfs" --group nfsgrp
pcs -f fs_cfg constraint colocation add nfsserver_data_sync nfsgrp INFINITY with-rsc-role=Master
pcs -f fs_cfg constraint order nfsserver_data_sync then start nfsgrp
pcs -f fs_cfg resource create nfsd nfsserver nfs_shared_infodir=/data/nfsinfo --group nfsgrp
pcs -f fs_cfg resource create nfsroot exportfs clientspec="*" options=rw,sync,no_root_squash directory=/data fsid=0 --group nfsgrp
pcs cluster cib-push fs_cfg
 
systemctl enable --now corosync
systemctl enable --now pacemaker
 
systemctl stop pcsd
systemctl start pcsd
systemctl start corosync
systemctl start pacemaker
 
 
Post Installation check on both nodes:
-------------------------------------
#pcs status
#pcs resource
#drbdadm status
 
Sample outputs:
--------------
 
[root@cluster1 ~]# pcs status
Cluster name: nfs_cluster
Stack: corosync
Current DC: cluster1 (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorum
Last updated: Tue Apr 26 17:41:55 2022
Last change: Tue Apr 26 16:27:15 2022 by root via cibadmin on cluster1
 
2 nodes configured
6 resource instances configured
 
Online: [ cluster1 cluster2 ]
 
Full list of resources:
 
nfs_vip        (ocf::heartbeat:IPaddr2):       Started cluster1
Master/Slave Set: nfsserver_data_sync [nfsserver_data]
     Masters: [ cluster1 ]
     Slaves: [ cluster2 ]
Resource Group: nfsgrp
     nfsfs      (ocf::heartbeat:Filesystem):    Started cluster1
     nfsd       (ocf::heartbeat:nfsserver):     Started cluster1
     nfsroot    (ocf::heartbeat:exportfs):      Started cluster1
 
Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
[root@cluster1 ~]# pcs resource
nfs_vip        (ocf::heartbeat:IPaddr2):       Started cluster1
Master/Slave Set: nfsserver_data_sync [nfsserver_data]
     Masters: [ cluster1 ]
     Slaves: [ cluster2 ]
Resource Group: nfsgrp
     nfsfs      (ocf::heartbeat:Filesystem):    Started cluster1
     nfsd       (ocf::heartbeat:nfsserver):     Started cluster1
     nfsroot    (ocf::heartbeat:exportfs):      Started cluster1
[root@cluster1 ~]# drbdadm status
d0 role:Primary
  disk:UpToDate
  cluster1 connection:StandAlone
 
[root@cluster1 ~]#
 
========================================================================
